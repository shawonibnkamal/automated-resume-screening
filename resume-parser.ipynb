{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ff76109",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pdfminer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_36428/3324363406.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpdfminer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhigh_level\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mextract_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdocx2txt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pdfminer'"
     ]
    }
   ],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "import docx2txt\n",
    "import nltk\n",
    "import re\n",
    "import subprocess "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc573bd",
   "metadata": {},
   "source": [
    "## Retrieve resume text from pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e783a8b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About Me\n",
      "\n",
      "Education\n",
      "\n",
      "134A University Ave, St. John's, A1B 1Z5, Canada\n",
      "(709) 986-7643  .  sikamal@mun.ca   . shawon.dev . github.com/shawonibnkamal\n",
      "\n",
      ".\n",
      "\n",
      "linkedin.com\n",
      "\n",
      "Shawon Ibn Kamal\n",
      "\n",
      "4th-year Computer Science student at Memorial University with\n",
      "over 2 years of experience as a professional software developer in\n",
      "Blue Communications Inc.\n",
      "\n",
      "BSc. Computer Science (Honours, 3.81 CGPA), Memorial University of\n",
      "Newfoundland, St. John's\n",
      "January 2018 - April 2022 (Expected)\n",
      "\n",
      "Employment History\n",
      "\n",
      "Programmer Analyst at Blue Communications Inc., St. John's\n",
      "July 2019 — Present\n",
      "\n",
      "● Design, develop, document, analyze, create, test, and modify computer systems,\n",
      "\n",
      "programs, and integrations by evaluating client needs.\n",
      "\n",
      "● Work with Javascript, Laravel, PHP, SQL, ASP.NET to build enterprise applications.\n",
      "● Manage projects through Git, CI/CD, and automate launching Ubuntu instances\n",
      "\n",
      "using AWS SDK.\n",
      "\n",
      "Data Programmer at Department of Psychology & Ecology, Memorial University of\n",
      "Newfoundland, St. John's\n",
      "September 2021 — Present\n",
      "\n",
      "● Robustify  the beta spectral data repository that will allow researchers to share their\n",
      "\n",
      "spectral datasets by maintaining a standard format.\n",
      "\n",
      "● Develop big data manipulation scripts using Python to standardize datasets which\n",
      "\n",
      "are validated by the spectral data repository.\n",
      "\n",
      "Full Stack Developer at South Park Driver Training, St. John's\n",
      "January 2021 — Present\n",
      "\n",
      "● Working on an existing web application using JS and PHP to build components for\n",
      "\n",
      "student log entries, LMS, billing calculation, and payroll for instructors.\n",
      "\n",
      "● Migrated projects to manage in AWS and optimized site pages to load fast and\n",
      "\n",
      "efﬁciently using fewer resources.\n",
      "\n",
      "● Fix any bug reports and continuously add new features as requested.\n",
      "\n",
      "Research Assistant at Department of Mathematics & Statistics, Memorial\n",
      "University of Newfoundland, St. John's\n",
      "January 2019 — April 2019\n",
      "\n",
      "● Analyzed delay differential equations from several mathematical research journals.\n",
      "● Determined stability and behavior of equations to compute bifurcation analysis\n",
      "\n",
      "using Matlab.\n",
      "\n",
      "\f",
      "Projects\n",
      "\n",
      "Share Notes\n",
      "December 2019 — Present\n",
      "\n",
      "● The successor of Shawon Notes which is an educational resource platform targeting\n",
      "\n",
      "the British curriculum of high schools reaching over 100k+ monthly page views.\n",
      "\n",
      "● Built in REST API format using React and Laravel to ensure scalability and\n",
      "\n",
      "modularity as I continue to add more features and scale up the project.\n",
      "\n",
      "Starcraft Competitive AI bot\n",
      "March 2021 — May 2021\n",
      "\n",
      "● Plays the game of Starcraft: Broodwar as Protoss race using real-time Artiﬁcial\n",
      "\n",
      "Intelligence decisions using C++ and BWAPI.\n",
      "\n",
      "● Beats the built-in AI of the game using the \"Zealot Rush\" strategy that includes\n",
      "scouting, appropriate building placement, proper build order, and timing attacks\n",
      "\n",
      "Game AI: Procedural Content Generation\n",
      "March 2021 — April 2021\n",
      "\n",
      "● Plays the game of Starcraft: Broodwar as Protoss race using real-time Artiﬁcial\n",
      "\n",
      "Intelligence decisions using C++ and BWAPI.\n",
      "\n",
      "● Beats the built-in AI of the game using the \"Zealot Rush\" strategy that includes\n",
      "scouting, appropriate building placement, proper build order, and timing attacks\n",
      "\n",
      "E-commerce Web Scraper\n",
      "January 2021 — April 2021\n",
      "\n",
      "● E-commerce web scraper built using Node.js and MongoDB that pulls the relevant\n",
      "\n",
      "information about items from Walmart and Best Buy and stores in the database.\n",
      "● It tracks the stock information, price changes, and availability on each update and\n",
      "\n",
      "notiﬁes the user about changes.\n",
      "\n",
      "Achievements\n",
      "\n",
      "Passed with Distinction in CICS Work Term with Blue Communications in criteria\n",
      "of Job Performance\n",
      "December 2020\n",
      "\n",
      "Received Spark Fund from Memorial University for getting started with the\n",
      "startup project Share Notes\n",
      "October 2020\n",
      "\n",
      "Skills\n",
      "\n",
      "Languages: Python, Java, Javascript, NodeJS, PHP, SQL, HTML, CSS, C, C++, R,\n",
      "Matlab\n",
      "\n",
      "Frameworks: ReactJS, Laravel, jQuery, MongoDB, Express, D3.js, SFML,\n",
      "Matplotlib, PIL, NumPy, SciPy, Pandas\n",
      "\n",
      "Other tools: Docker, Linux Server, AWS, SMTP, S3, Apache, REST APIs, Visual\n",
      "Studio, Photoshop\n",
      "\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "\n",
    "\n",
    "text = extract_text_from_pdf('Resume.pdf')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04186c9",
   "metadata": {},
   "source": [
    "## Retrieve candidate name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "025a2c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "def extract_names(txt):\n",
    "    person_names = []\n",
    "\n",
    "    for sent in nltk.sent_tokenize(txt):\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            output.append(chunk)\n",
    "            if hasattr(chunk, 'label') and chunk.label() == 'PERSON':\n",
    "                person_names.append(\n",
    "                    ' '.join(chunk_leave[0] for chunk_leave in chunk.leaves())\n",
    "                )\n",
    "\n",
    "    return person_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a98f8e44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Me', 'John', 'Shawon Ibn Kamal', 'Honours', 'Javascript', 'Laravel', 'Data', 'Ecology', 'Develop', 'Python', 'Full', 'Stack Developer', 'Matlab', 'Share', 'Shawon Notes', 'React', 'Laravel', 'Starcraft', 'Starcraft', 'Broodwar', 'Protoss', 'Zealot Rush', 'Starcraft', 'Broodwar', 'Protoss', 'Zealot Rush', 'Scraper', 'Best Buy', 'Spark Fund', 'Share', 'Python', 'Java', 'Javascript', 'Matlab Frameworks', 'Laravel', 'Express', 'Matplotlib', 'Pandas', 'Docker', 'Linux Server', 'Apache', 'Visual Studio', 'Photoshop']\n"
     ]
    }
   ],
   "source": [
    "names = extract_names(text)\n",
    "if names:\n",
    "    print(names)\n",
    "#output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cd5e33",
   "metadata": {},
   "source": [
    "## Extract phone-number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d87ec2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(709) 986-7643\n"
     ]
    }
   ],
   "source": [
    "PHONE_REG = re.compile(r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]')\n",
    "\n",
    "def extract_phone_number(resume_text):\n",
    "    phone = re.findall(PHONE_REG, resume_text)\n",
    "\n",
    "    if phone:\n",
    "        number = ''.join(phone[0])\n",
    "\n",
    "        if resume_text.find(number) >= 0 and len(number) < 16:\n",
    "            return number\n",
    "    return None\n",
    "\n",
    "\n",
    "phone_number = extract_phone_number(text)\n",
    "print(phone_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538a7424",
   "metadata": {},
   "source": [
    "## Extract email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8bbb2ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sikamal@mun.ca\n"
     ]
    }
   ],
   "source": [
    "EMAIL_REG = re.compile(r'[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+')\n",
    "def extract_emails(resume_text):\n",
    "    return re.findall(EMAIL_REG, resume_text)\n",
    "\n",
    "emails = extract_emails(text)\n",
    "if emails:\n",
    "    print(emails[0])  # noqa: T001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e8b82a",
   "metadata": {},
   "source": [
    "## Extract school"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f0d0e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Memorial University', 'University Ave'}\n"
     ]
    }
   ],
   "source": [
    "RESERVED_WORDS = [\n",
    "    'school',\n",
    "    'college',\n",
    "    'univers',\n",
    "    'academy',\n",
    "    'faculty',\n",
    "    'institute',\n",
    "    'faculdades',\n",
    "    'Schola',\n",
    "    'schule',\n",
    "    'lise',\n",
    "    'lyceum',\n",
    "    'lycee',\n",
    "    'polytechnic',\n",
    "    'kolej',\n",
    "    'ünivers',\n",
    "    'okul',\n",
    "]\n",
    "\n",
    "def extract_education(input_text):\n",
    "    organizations = []\n",
    "\n",
    "    # first get all the organization names using nltk\n",
    "    for sent in nltk.sent_tokenize(input_text):\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            if hasattr(chunk, 'label') and chunk.label() == 'ORGANIZATION':\n",
    "                organizations.append(' '.join(c[0] for c in chunk.leaves()))\n",
    "\n",
    "    # we search for each bigram and trigram for reserved words\n",
    "    # (college, university etc...)\n",
    "    education = set()\n",
    "    for org in organizations:\n",
    "        for word in RESERVED_WORDS:\n",
    "            if org.lower().find(word) >= 0:\n",
    "                education.add(org)\n",
    "\n",
    "    return education\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "education_information = extract_education(text)\n",
    "print(education_information)  # noqa: T001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99a9641",
   "metadata": {},
   "source": [
    "## Extract Job Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef1d74d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'developer', 'Developer', 'Programmer Analyst', 'Full Stack Developer'}\n"
     ]
    }
   ],
   "source": [
    "# you may read the database from a csv file or some other database\n",
    "SKILLS_DB = [\n",
    "    'programmer analyst',\n",
    "    'data science',\n",
    "    'developer',\n",
    "    'full stack developer',\n",
    "    'excel',\n",
    "    'English',\n",
    "    'data scientist',\n",
    "    'accountant',\n",
    "]\n",
    "\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    txt = docx2txt.process(docx_path)\n",
    "    if txt:\n",
    "        return txt.replace('\\t', ' ')\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_skills(input_text):\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    word_tokens = nltk.tokenize.word_tokenize(input_text)\n",
    "\n",
    "    # remove the stop words\n",
    "    filtered_tokens = [w for w in word_tokens if w not in stop_words]\n",
    "\n",
    "    # remove the punctuation\n",
    "    filtered_tokens = [w for w in word_tokens if w.isalpha()]\n",
    "\n",
    "    # generate bigrams and trigrams (such as artificial intelligence)\n",
    "    bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtered_tokens, 2, 3)))\n",
    "\n",
    "    # we create a set to keep the results in.\n",
    "    found_skills = set()\n",
    "\n",
    "    # we search for each token in our skills database\n",
    "    for token in filtered_tokens:\n",
    "        if token.lower() in SKILLS_DB:\n",
    "            found_skills.add(token)\n",
    "\n",
    "    # we search for each bigram and trigram in our skills database\n",
    "    for ngram in bigrams_trigrams:\n",
    "        if ngram.lower() in SKILLS_DB:\n",
    "            found_skills.add(ngram)\n",
    "\n",
    "    return found_skills\n",
    "\n",
    "\n",
    "skills = extract_skills(text)\n",
    "print(skills)  # noqa: T001"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
